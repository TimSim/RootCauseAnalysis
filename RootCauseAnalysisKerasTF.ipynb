{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root Cause Analysis of IT incedents\n",
    "\n",
    "Automatic identification of the root cause for a system issue empowers level two technicians while reducing escalations, reduces resolution times, improving user satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.3\n",
      "  latest version: 4.9.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Install necessary packages in conda. Use pip in not using conda.\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} pandas tensorflow scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Incident Data - Loading the Dataset\n",
    "1. Load the CSV into a Pandas data frame using the read_csv method. \n",
    "2. Print the data types and contents of the data frame to verify load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID              int64\n",
      "CPU_LOAD        int64\n",
      "MEMORY_LOAD     int64\n",
      "DELAY           int64\n",
      "ERROR_1000      int64\n",
      "ERROR_1001      int64\n",
      "ERROR_1002      int64\n",
      "ERROR_1003      int64\n",
      "ROOT_CAUSE     object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CPU_LOAD</th>\n",
       "      <th>MEMORY_LOAD</th>\n",
       "      <th>DELAY</th>\n",
       "      <th>ERROR_1000</th>\n",
       "      <th>ERROR_1001</th>\n",
       "      <th>ERROR_1002</th>\n",
       "      <th>ERROR_1003</th>\n",
       "      <th>ROOT_CAUSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MEMORY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MEMORY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>MEMORY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MEMORY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NETWORK_DELAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  CPU_LOAD  MEMORY_LOAD  DELAY  ERROR_1000  ERROR_1001  ERROR_1002  \\\n",
       "0   1         0            0      0           0           1           0   \n",
       "1   2         0            0      0           0           0           0   \n",
       "2   3         0            1      1           0           0           1   \n",
       "3   4         0            1      0           1           1           0   \n",
       "4   5         1            1      0           1           0           1   \n",
       "\n",
       "   ERROR_1003     ROOT_CAUSE  \n",
       "0           1         MEMORY  \n",
       "1           1         MEMORY  \n",
       "2           1         MEMORY  \n",
       "3           1         MEMORY  \n",
       "4           0  NETWORK_DELAY  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "#Load the data file into a Pandas Dataframe\n",
    "symptom_data = pd.read_csv(\"root_cause_analysis.csv\")\n",
    "\n",
    "#Take a look at the data loaded\n",
    "print(symptom_data.dtypes)\n",
    "symptom_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data\n",
    "Keras works with NumPy arrays. We need to convert the data accordingly. \n",
    "1. Convert root cause into a numeric value.\n",
    "2. Convert the data frame into a NumPy array using the to_numpy function.\n",
    "3. Separate the training attributes features (F_train) into the target (T_train) array. \n",
    "4. Extract target var into T_train.\n",
    "5. Use the one-hot encoding on this categorical target attribute for it to be consumed by Keras - see utils.to_categorical function.\n",
    "6. Print the shape of these variables to check if all the processing was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples and shape of feature variables : (1000, 7)\n",
      "Number of samples and shape of target variable : (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#convert from text to numeric values\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "symptom_data['ROOT_CAUSE'] = label_encoder.fit_transform(\n",
    "                                symptom_data['ROOT_CAUSE'])\n",
    "\n",
    "#Convert Pandas DataFrame to a numpy vector\n",
    "np_symptom = symptom_data.to_numpy().astype(float)\n",
    "\n",
    "#Extract the feature variables (F)\n",
    "F_train = np_symptom[:,1:8]\n",
    "\n",
    "#Extract the target variable (T), convert to one-hot-encoding\n",
    "T_train=np_symptom[:,8]\n",
    "T_train = tf.keras.utils.to_categorical(T_train,3)\n",
    "\n",
    "print(\"Number of samples and shape of feature variables :\", F_train.shape)\n",
    "print(\"Number of samples and shape of target variable :\",T_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model with Keras\n",
    "\n",
    "Create a softmax model with three dense layers in Keras to predict the root cause based on the symptoms. \n",
    "\n",
    "1. Set up the hyperparameters for the neural network. \n",
    "2. Use Keras sequential model which is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
    "3. The number of output classes will be set to match the unique number of labels in the target variable, namely root cause. \n",
    "4. Add a hidden layer size of 128. \n",
    "5. Validate with 20% of the input records. \n",
    "6. Create the sequential model. \n",
    "7. Add the first hidden layer as a dense layer with activation as RLU. \n",
    "8. Add a second similar layer. \n",
    "9. Add a softmax layer to provide categorical labels. \n",
    "10. Compile the model (configure its learning process) using Adam optimizer and set the loss function as categorical, cross, and trophy. \n",
    "11. Fit the model with F_train and T_train in order to create the model. \n",
    "12. Print the model summary to see the model structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/20\n",
      "800/800 [==============================] - 1s 902us/sample - loss: 1.0196 - accuracy: 0.6075 - val_loss: 0.9169 - val_accuracy: 0.7700\n",
      "Epoch 2/20\n",
      "800/800 [==============================] - 0s 82us/sample - loss: 0.8389 - accuracy: 0.7950 - val_loss: 0.7675 - val_accuracy: 0.7600\n",
      "Epoch 3/20\n",
      "800/800 [==============================] - 0s 84us/sample - loss: 0.6842 - accuracy: 0.8112 - val_loss: 0.6537 - val_accuracy: 0.7800\n",
      "Epoch 4/20\n",
      "800/800 [==============================] - 0s 85us/sample - loss: 0.5661 - accuracy: 0.8188 - val_loss: 0.5800 - val_accuracy: 0.7900\n",
      "Epoch 5/20\n",
      "800/800 [==============================] - 0s 87us/sample - loss: 0.4955 - accuracy: 0.8225 - val_loss: 0.5408 - val_accuracy: 0.7700\n",
      "Epoch 6/20\n",
      "800/800 [==============================] - 0s 107us/sample - loss: 0.4582 - accuracy: 0.8313 - val_loss: 0.5276 - val_accuracy: 0.8000\n",
      "Epoch 7/20\n",
      "800/800 [==============================] - 0s 99us/sample - loss: 0.4411 - accuracy: 0.8413 - val_loss: 0.5223 - val_accuracy: 0.8000\n",
      "Epoch 8/20\n",
      "800/800 [==============================] - 0s 110us/sample - loss: 0.4321 - accuracy: 0.8413 - val_loss: 0.5203 - val_accuracy: 0.8000\n",
      "Epoch 9/20\n",
      "800/800 [==============================] - 0s 96us/sample - loss: 0.4220 - accuracy: 0.8400 - val_loss: 0.5225 - val_accuracy: 0.8200\n",
      "Epoch 10/20\n",
      "800/800 [==============================] - 0s 111us/sample - loss: 0.4176 - accuracy: 0.8525 - val_loss: 0.5124 - val_accuracy: 0.7900\n",
      "Epoch 11/20\n",
      "800/800 [==============================] - 0s 96us/sample - loss: 0.4118 - accuracy: 0.8462 - val_loss: 0.5127 - val_accuracy: 0.8200\n",
      "Epoch 12/20\n",
      "800/800 [==============================] - 0s 92us/sample - loss: 0.4041 - accuracy: 0.8600 - val_loss: 0.5101 - val_accuracy: 0.8100\n",
      "Epoch 13/20\n",
      "800/800 [==============================] - 0s 93us/sample - loss: 0.4000 - accuracy: 0.8550 - val_loss: 0.5048 - val_accuracy: 0.8000\n",
      "Epoch 14/20\n",
      "800/800 [==============================] - 0s 93us/sample - loss: 0.3959 - accuracy: 0.8500 - val_loss: 0.5013 - val_accuracy: 0.8100\n",
      "Epoch 15/20\n",
      "800/800 [==============================] - 0s 93us/sample - loss: 0.3899 - accuracy: 0.8587 - val_loss: 0.5084 - val_accuracy: 0.8300\n",
      "Epoch 16/20\n",
      "800/800 [==============================] - 0s 90us/sample - loss: 0.3843 - accuracy: 0.8612 - val_loss: 0.4982 - val_accuracy: 0.8000\n",
      "Epoch 17/20\n",
      "800/800 [==============================] - 0s 91us/sample - loss: 0.3816 - accuracy: 0.8587 - val_loss: 0.4945 - val_accuracy: 0.8300\n",
      "Epoch 18/20\n",
      "800/800 [==============================] - 0s 96us/sample - loss: 0.3772 - accuracy: 0.8600 - val_loss: 0.4941 - val_accuracy: 0.8200\n",
      "Epoch 19/20\n",
      "800/800 [==============================] - 0s 124us/sample - loss: 0.3786 - accuracy: 0.8550 - val_loss: 0.4925 - val_accuracy: 0.8000\n",
      "Epoch 20/20\n",
      "800/800 [==============================] - 0s 127us/sample - loss: 0.3728 - accuracy: 0.8537 - val_loss: 0.4901 - val_accuracy: 0.8200\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Dense-Layer-1 (Dense)        (None, 128)               1024      \n",
      "_________________________________________________________________\n",
      "Dense-Layer-2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "Final (Dense)                (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 17,923\n",
      "Trainable params: 17,923\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "#Training Parameters\n",
    "EPOCHS=20\n",
    "BATCH_SIZE=100\n",
    "VERBOSE=1\n",
    "OUTPUT_CLASSES=len(label_encoder.classes_)\n",
    "N_HIDDEN=128\n",
    "VALIDATION_SPLIT=0.2\n",
    "\n",
    "#Create a Keras sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "#Add a Dense Layer\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "                             input_shape=(7,),\n",
    "                              name='Dense-Layer-1',\n",
    "                              activation='relu'))\n",
    "\n",
    "#Add a second dense layer\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "                              name='Dense-Layer-2',\n",
    "                              activation='relu'))\n",
    "\n",
    "#Add a softmax layer for categorial prediction\n",
    "model.add(keras.layers.Dense(OUTPUT_CLASSES,\n",
    "                             name='Final',\n",
    "                             activation='softmax'))\n",
    "\n",
    "#Compile the model, using Adam optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Build the model\n",
    "model.fit(F_train,\n",
    "          T_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=VERBOSE,\n",
    "          validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Root Causes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DATABASE_ISSUE']\n"
     ]
    }
   ],
   "source": [
    "#Pass individual flags to Predict the root cause\n",
    "CPU_LOAD=1\n",
    "MEMORY_LOAD=0\n",
    "DELAY=0\n",
    "ERROR_1000=0\n",
    "ERROR_1001=1\n",
    "ERROR_1002=1\n",
    "ERROR_1003=0\n",
    "\n",
    "prediction=model.predict_classes(\n",
    "    [[CPU_LOAD,MEMORY_LOAD,DELAY,\n",
    "      ERROR_1000,ERROR_1001,ERROR_1002,ERROR_1003]])\n",
    "\n",
    "print(label_encoder.inverse_transform(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DATABASE_ISSUE' 'NETWORK_DELAY' 'MEMORY' 'DATABASE_ISSUE'\n",
      " 'DATABASE_ISSUE']\n"
     ]
    }
   ],
   "source": [
    "#Predicting as a Batch\n",
    "print(label_encoder.inverse_transform(\n",
    "        model.predict_classes([[1,0,0,0,1,1,0],\n",
    "                                [0,1,1,1,0,0,0],\n",
    "                                [1,1,0,1,1,0,1],\n",
    "                                [0,0,0,0,0,1,0],\n",
    "                                [1,0,1,0,1,1,1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
